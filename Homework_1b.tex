\documentclass[12pt]{amsart}

\usepackage[margin=3.5 cm]{geometry}
\usepackage{graphicx, mathabx}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\graphicspath{ {./images/} }
\linespread{1.5}
\newcommand{\vect}[1]{\mathbf{#1}}
\begin{document}
\title[STAT339 - Machine Learning]{STAT 339 - Homework 1b (Linear Regressions)}

\author{Peng Gu, Liam Axon, Xiaoyun Gong} 
\date{\today}

\maketitle

\section*{Exercise 1}

\subsection*{part(b)}

We applied our own OLS solver to the ``womens100.csv'' data set, with the results shown below:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{Figure_womesn100.png}
    \caption{Graph of ``womens100.csv'' best fit}
    \label{fig:my_label}
\end{figure}

The line shown on the graph is $y = w_0 + w_1 x$ where $w_0 = 40.09$ and $w_1 = -0.015$.

We compared this to the line given in the textbook, which has $w_0 = 40.92$ and $w_1 = -0.015$.

These values are very close, and tell us that our regression solver is correctly finding a line of best fit.

\subsection*{part(c)}

Our algorithm predicted a value for the 2012 Olympics of 10.647, and it predicted a value for the 2016 Olympics of 10.593s. The actual racetime for 2012 is 10.75s, and the actual racetime for 2016 is 10.71s. These are both a little high of the actual value, but the difference is not prominent.

In fact, the squared prediction error for 2012 is 0.0106 and the squared prediction error for 2016 is 0.0136.

\subsection*{part(g)}

When we applied our polynomial regression solver to the ``synthdata2016.csv'' data set, looking for a cubic of best fit, we got the following:

\begin{figure}[H]
    \centering
    \includegraphics{Figure_synthdata.png}
    \caption{Graph of ``synthdata2016.csv'' best fit}
    \label{fig:my_label}
\end{figure}

\newpage
\section*{Exercise 2}
\subsection*{part(d)}
For womens100.csv, when K equals 10, optimal polynomial order according to the validation set is 2. 
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{K=10_women.png}
    \caption{Validation error for ``womens100.csv''(K=10)}
    \label{fig:my_label}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{K=10_women_t.png}
    \caption{Train error for ``womens100.csv''(K=10)}
    \label{fig:my_label}
\end{figure}
When K equals N, optimal polynomial order according to the validation set is 2. The situation for the two K values is similar: it is hard to tell that one polynomial order is significantly better than the other due to the high standard deviation range in the validation error graph, however the training error graph makes sense since the graph can nearly perfectly fit every point when the order approaches the number of points, and possibly the original curve has polynomial around 2.5 so that's where training error was lowest.\\
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{K=N_women.png}
    \caption{Validation error for ``womens100.csv''(K=N)}
    \label{fig:my_label}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{K=N_women_t.png}
    \caption{Train error for ``womens100.csv''(K=N)}
    \label{fig:my_label}
\end{figure}
$ $\\
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{K=10_synth.png}
    \caption{Validation error for ``womens100.csv''(K=10)}
    \label{fig:my_label}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{K=10_synth_t.png}
    \caption{Train error for ``womens100.csv''(K=10)}
    \label{fig:my_label}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{K=N_synth.png}
    \caption{Validation error for ``womens100.csv''(K=N)}
    \label{fig:my_label}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.7]{K=N_synth_t.png}
    \caption{Train error for ``womens100.csv''(K=N)}
    \label{fig:my_label}
\end{figure}
For synthdata2016.csv, when K equals 10, optimal order is 4 according to the validation error graph. When K equals N, optimal order is 12. Both K values give very strange looking training error graphs. This might be due to the size of the dataset. Validation error graphs are reasonable similar to those for womens100.csv.

\subsection*{part(e)}

\subsection*{part(f)}
Grid search $\lambda$ from 1 to 16 and D from 1 to 16. For synthedata2016.csv, optimal pair is $\lambda = 5$ and $D=1$. Error for OLS is 0.05985, error for ridge regression is 0.062429. For womens100.csv, optimal pair is $\lambda = 2$ and $D=1$. Error for OLS is 0.0002554 and error for ridge regression is 0.0030068. It seems that ridge regression requires more fine tuning in hyperparameteres, probably $\lambda$ and D with shorter intervals within a larger range. With bad hyperparameters it is easily outperformed by OLS.

\newpage

\section*{Exercise 3}
\subsection*{part(a)}
\begin{proof}
Let $f,g:\mathbb{R}^N\to\mathbb{R}^M$,  $\vect{v}\in\mathbb{R}^N$, and $a = f(\vect{v})^T g(\vect{v})\in \mathbb{R}$.
\begin{align*}
    \frac{da}{d\vect{v}} &= \frac{d}{d\vect{v}}(f(\vect{v})^T g(\vect{v}))\\
    &= \frac{d}{d\vect{v}} \sum_{i=1}^M (f_i(\vect{v})g_i(\vect{v}))\\
    &= \sum_{i=1}^M \frac{d}{d\vect{v}}(f_i(\vect{v})g_i(\vect{v}))\\
    &= \sum_{i=1}^M (f_i(\vect{v})\frac{dg_i}{d\vect{v}}+g_i(\vect{v})\frac{df_i}{d\vect{v}})\\
    &= \sum_{i=1}^M f_i(\vect{v})\frac{dg_i}{d\vect{v}} +\sum_{i=1}^M g_i(\vect{v})\frac{df_i}{d\vect{v}}\\
    &= f(\vect{v})^T\frac{dg}{d\vect{v}} +g(\vect{v})^T\frac{df}{d\vect{v}}
\end{align*}
\end{proof}

\subsection*{part(b)}
\begin{proof}
Let $g:\mathbb{R}^N\to\mathbb{R}^N$, $g(\vect{v})=\vect{A}\vect{v}$, and let $f=\textbf{I}$.
\begin{align*}
    \frac{d}{d\vect{v}}\vect{v}^T\vect{A}\vect{v}
    = \frac{d}{d\vect{v}}f(\vect{v})^T g(\vect{v})
    &=f(\vect{v})^T \frac{dg}{d\vect{v}}
    + g(\vect{v})^T  \frac{df}{d\vect{v}}\\
    &= f(\vect{v})^T \vect{A}
    +g(\vect{v})^T\vect{I}\\
&= \vect{v}^T\vect{A}+(\vect{A}\vect{v})^T\\
&= \vect{v}^T\vect{A} + \vect{v}^T\vect{A}^T\\
&= \vect{v}^T\vect{A} + \vect{v}^T\vect{A}\\
&= 2\vect{v}^T\vect{A}
\end{align*}
\end{proof}
\section*{Exercise 4}
We need to find the derivative first:
\begin{align*}
   \frac{d\mathcal{L}}{d\vect{w}}&=\frac{1}{N}[
   (\vect{t}-\vect{Xw})^T \frac{d}{d\vect{w}}(\vect{A(t-Xw)}) +
   (\vect{A(t-Xw)})^T \frac{d}{d\vect{w}}(\vect{t}-\vect{Xw})
   ]\\
   &= \frac{1}{N}[(\vect{t}-\vect{Xw})^T(-\vect{AX})+ \vect{(t-Xw)}^T\vect{A}^T(-\vect{X})]\\
   &\mbox{(Note that $\vect{A}$ is symmetric, so $\vect{A}^T = \vect{A}$.)}\\
   & = -\frac{1}{N}[\vect{(t-Xw)}^T\vect{AX}+\vect{(t-Xw)}^T\vect{AX}]\\
   & = -\frac{2}{N}[\vect{(t-Xw)}^T\vect{AX}]
\end{align*}
Now, set $ \frac{d\mathcal{L}}{d\vect{w}}$ to $0$:
\begin{align*}
    -\frac{2}{N}[\vect{(t-Xw)}^T\vect{AX}]&=0\\
    \vect{(t-Xw)}^T\vect{AX}&=0\\
    (\vect{t}^T-\vect{w}^T\vect{X}^T)\vect{AX}&=0\\
    \vect{t}^T\vect{AX}-\vect{w}^T\vect{X}^T\vect{AX}&=0\\
    \vect{w}^T\vect{X}^T\vect{AX} &=\vect{t}^T\vect{AX}\\
    \vect{w}^T&=\vect{t}^T\vect{AX}(\vect{X}^T\vect{AX})^{-1}\\
    \vect{w} &= ((\vect{X}^T\vect{AX})^{-1})^{T}\vect{X}^T\vect{A}^T\vect{t}\\
    \vect{w} &= ((\vect{X}^T\vect{AX})^{T})^{-1}\vect{X}^T\vect{A}^T\vect{t}\\
    \mbox{(Again, note that $\vect{A}$}&\mbox{  is symmetric, so $\vect{A}^T = \vect{A}$.)}\\
    \vect{w} &=(\vect{X}^T\vect{AX})^{-1}\vect{X}^T\vect{A}\vect{t}
\end{align*}
\end{document}

